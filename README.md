# CL-2-Project
## Sentiment Analysis on Social Media Texts with Semantic Interpretation

A comparative study combining **lexicon-based semantic features** with **machine learning approaches** for sentiment classification on Twitter data.

This project explores how integrating **semantic interpretation techniques**â€”such as negation handling, intensifiers, and pre-trained embeddingsâ€”can improve sentiment analysis performance compared to traditional lexical methods.

**Team Members:**
- **Naman:** Data Pipeline & Traditional ML (Phase 1 Complete)
- **Shrish:** Semantic Features & Advanced ML (Phase 1 Complete)

---

## Project Overview

The project aims to evaluate how combining **semantic features** with standard text-based representations affects the accuracy and robustness of sentiment classification models on social media text.

**Key Objectives:**
- Develop preprocessing pipeline for noisy Twitter data
- Engineer semantic, contextual, and traditional lexical features
- Compare model performance with and without semantic enrichment
- Conduct feature ablation, error analysis, and qualitative interpretation

---

## Quick Start

### Installation

```bash
# Install required dependencies
pip install -r requirements.txt
```

### Test Integration

```bash
# Test that all modules work together
python test_integration.py
```

### Run Preprocessing Pipeline

```bash
# Process both datasets with default settings (80-20 split)
python scripts/main.py

# Process only Sentiment140 with custom sample size
python scripts/main.py --dataset sentiment140 --sample-size 10000

# Process full datasets
python scripts/process_full_data.py
```

---

## Project Structure

```
CL-2-Project/
â”œâ”€â”€ preprocessing/                 # Data preprocessing (Naman - Phase 1)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing.py          # TweetPreprocessor class
â”‚   â””â”€â”€ data_loader.py            # Data loading and splitting
â”‚
â”œâ”€â”€ features/                      # Feature extraction (Shrish - Phase 1)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ contextual_features.py    # Negation, intensifiers, emphasis
â”‚   â”œâ”€â”€ semantic_embeddings.py    # Word2Vec, GloVe embeddings
â”‚   â”œâ”€â”€ lexicon_scoring.py        # VADER, NRC emotion lexicons
â”‚   â””â”€â”€ feature_pipeline.py       # Unified feature extraction
â”‚
â”œâ”€â”€ scripts/                       # Execution scripts
â”‚   â”œâ”€â”€ main.py                   # Main preprocessing pipeline
â”‚   â””â”€â”€ process_full_data.py      # Full dataset processor
â”‚
â”œâ”€â”€ datasets/                      # Raw datasets (not tracked)
â”‚   â”œâ”€â”€ Sentiment140_dataset/
â”‚   â””â”€â”€ cross_validation_dataset/
â”‚
â”œâ”€â”€ test_integration.py           # Integration tests
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md                     # This file
```

---

## Phase 1 Implementation (Complete âœ“)

### Preprocessing Module (Naman)
- âœ… **Text Cleaning:** URL, mention, hashtag removal, HTML entity decoding
- âœ… **Tokenization:** NLTK's TweetTokenizer for Twitter-specific text
- âœ… **Normalization:** Stopword removal and lemmatization
- âœ… **Emoji Preservation:** Retains emojis and emoticons for sentiment context
- âœ… **Train-Test Split:** Stratified 80-20 split with class balancing
- âœ… **Reproducibility:** Fixed random seed (42) for consistent results

### Feature Extraction Module (Shrish)
- âœ… **Contextual Features:** 
  - Negation detection and scope analysis
  - Intensifier and diminisher identification
  - Emphasis pattern detection (caps, exclamations, etc.)
  - Twitter-specific features (mentions, hashtags, retweets)
  
- âœ… **Semantic Embeddings:**
  - Word2Vec (Google News 300d)
  - GloVe (Wiki Gigaword 300d)
  - Sentiment similarity scoring
  - Dimensionality reduction (PCA)
  
- âœ… **Lexicon-Based Scoring:**
  - VADER sentiment analyzer
  - NRC Emotion Lexicon (10 emotions)
  - Custom polarity scoring
  - Sentiment modifiers

- âœ… **Feature Pipeline:**
  - Unified extraction interface
  - Configurable feature types
  - Feature scaling and normalization
  - Save/load capabilities

---

## Usage Examples

### Python API - Preprocessing

```python
from preprocessing import TweetPreprocessor, SentimentDataLoader

# Preprocess individual tweets
preprocessor = TweetPreprocessor()
processed = preprocessor.preprocess("@user This is amazing! ğŸ˜ #happy")
# Output: "amazing ğŸ˜ happy"

# Load and preprocess datasets
loader = SentimentDataLoader('datasets')
df = loader.load_sentiment140(sample_size=10000)
train_df, test_df = loader.create_train_test_split(df, test_size=0.2)
```

### Python API - Feature Extraction

```python
from features import (
    ContextualFeatures,
    LexiconBasedScoring,
    SemanticEmbeddings,
    FeatureExtractionPipeline
)

# Extract contextual features
contextual = ContextualFeatures()
features = contextual.extract_contextual_features("Not bad at all!")

# Extract lexicon features
lexicon = LexiconBasedScoring()
lexicon.initialize_lexicons()
scores = lexicon.extract_lexicon_features("This is amazing!")

# Unified feature pipeline
pipeline = FeatureExtractionPipeline()
pipeline.initialize_extractors()
all_features = pipeline.extract_all_features(["Sample tweet"])
```

---

## Datasets

### **Primary Dataset: Sentiment140**
- **Source:** [Kaggle â€“ Sentiment140 Dataset](https://www.kaggle.com/datasets/kazanova/sentiment140)
- **Description:** 1.6M labeled tweets
- **Labels:** Binary (0=negative, 4=positive â†’ mapped to 0 and 1)
- **Purpose:** Model training and primary evaluation
- **Split:** 80% training, 20% testing (stratified)

### **Secondary Dataset: Twitter US Airline Sentiment**
- **Source:** [Kaggle â€“ US Airline Sentiment Dataset](https://www.kaggle.com/datasets/crowdflower/twitter-airline-sentiment)
- **Description:** 14.6K labeled tweets about airline services
- **Labels:** Negative (0), Neutral (1), Positive (2)
- **Purpose:** Cross-domain validation and generalization testing

---

## Key Features

### Preprocessing
âœ“ **Emoji preservation**: ğŸ˜ ğŸ˜¡ ğŸ‰ are retained  
âœ“ **Emoticon preservation**: :) :( <3 are retained  
âœ“ **URL removal**: http links stripped  
âœ“ **Mention removal**: @username removed  
âœ“ **Hashtag processing**: #happy â†’ happy  
âœ“ **HTML decoding**: &amp; â†’ &  
âœ“ **Stopword removal**: common words removed  
âœ“ **Lemmatization**: running â†’ run  
âœ“ **Stratified splits**: class balance maintained  
âœ“ **Reproducible**: fixed random seed

### Feature Extraction
âœ“ **12 contextual features**: negation, intensifiers, emphasis  
âœ“ **22 lexicon features**: VADER, NRC emotions, custom scoring  
âœ“ **34 semantic features**: Word2Vec, GloVe, sentiment similarity  
âœ“ **Traditional features**: TF-IDF, BoW, POS tags (configurable)  
âœ“ **Unified pipeline**: easy integration and scaling  

---

## Testing

```bash
# Run integration tests
python test_integration.py

# Expected output:
# âœ“ Preprocessing: PASS
# âœ“ Features: PASS  
# âœ“ Integration: PASS
# âœ“ ALL TESTS PASSED!
```

---

## Next Steps (Phase 2)

**Naman:**
- [ ] Implement N-gram feature extraction
- [ ] Train Naive Bayes classifier
- [ ] Train Logistic Regression
- [ ] Set up cross-validation framework
- [ ] Initial model evaluation

**Shrish:**
- [ ] Train Random Forest classifier
- [ ] Feature ablation study
- [ ] Cross-domain validation
- [ ] Comparative analysis

---

## Dependencies

```
numpy>=1.21.0          # Numerical operations
pandas>=1.3.0          # Data manipulation
scikit-learn>=1.0.0    # Machine learning
nltk>=3.6.0            # NLP toolkit
gensim>=4.0.0          # Word embeddings
vaderSentiment>=3.3.2  # Sentiment analysis
```

---

## License

This project is for academic purposes as part of CL-2 coursework.

---

## Acknowledgments

- Sentiment140 dataset creators
- VADER sentiment analysis tool
- NRC Emotion Lexicon
- Word2Vec and GloVe embedding projects
